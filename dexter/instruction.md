Краткое руководство по работе с DGX A100 (Dexter) через Slurm

  Добро пожаловать! Эта инструкция поможет вам запускать ваши задачи машинного обучения на нашем сервере. Мы используем систему управления очередями Slurm, которая обеспечивает справедливое распределение ресурсов (GPU, CPU, RAM) между всеми пользователями.

  1. Что такое Slurm и почему мы его используем?

  Представьте, что сервер — это ресторан с ограниченным числом столов (ресурсов). Если каждый будет занимать любой стол на неограниченное время, возникнет хаос.

  Slurm — это "метрдотель" нашего сервера. Вы не запускаете расчет напрямую, а оформляете "заказ" (скрипт), в котором указываете, какие ресурсы вам нужны (например, 1 GPU, 8 ядер CPU, 64 ГБ памяти). Slurm ставит ваш заказ в очередь и, как только нужные ресурсы освободятся, запускает ваш расчет.

  Ключевые преимущества:
   * Справедливость: Никто не может "захватить" весь сервер.
   * Эффективность: Ресурсы не простаивают.
   * Воспроизводимость: Ваши запуски всегда выполняются в одинаковых, контролируемых условиях.

  2. Базовые команды Slurm

   * squeue — посмотреть текущую очередь задач (ваших и чужих).
   * sbatch <имя_скрипта.sbatch> — отправить ваш "заказ" (скрипт) в очередь.
   * scancel <ID_задачи> — отменить вашу задачу (ID можно узнать из squeue).
   * sinfo — посмотреть состояние разделов (partitions) и узлов кластера.

  3. Создание вашего Python-окружения (Ключевой шаг!)

  На нашем сервере используется специфическая конфигурация оборудования и драйверов. Чтобы ваши расчеты работали корректно и с максимальной производительностью, крайне важно правильно создать ваше Conda-окружение.
  Проблема: Простая установка (conda install pytorch) может привести к несовместимости версий и трудноуловимым ошибкам во время расчетов.
  Решение: Мы экспериментально подобрали рабочий набор пакетов. Используйте его как основу для своих окружений.
  Шаги по созданию окружения:
   1. Перейдите в свою рабочую директорию (например, cd ~/test). Там уже должен лежать файл torch-env-portable.yml.
   2. Создайте окружение командой:

        conda env create -f torch-env-portable.yml -p ./my-torch-env
       * -f torch-env-portable.yml — указывает использовать проверенный файл конфигурации.
       * -p ./my-torch-env — создает окружение в подпапке my-torch-env внутри вашей текущей директории. Это удобно для изоляции проектов.

  Этот процесс может занять несколько минут.

  4. Запуск тестового расчета

   1. Проанализируйте скрипт `run_test_conda_1gpu.sbatch`. Откройте его и посмотрите на строки, начинающиеся с #SBATCH:
       * #SBATCH --gpus=1: Запрос одного GPU.
       * #SBATCH --cpus-per-task=8: Запрос 8 ядер CPU.
       * #SBATCH --mem=64G: Запрос 64 ГБ оперативной памяти.
       * conda activate ./torch-env В скрипте указан путь к окружению. Не забудьте поменять его на свой: conda activate ./my-torch-env.

   2. Запустите задачу:
        sbatch run_test_conda_1gpu.sbatch

   3. Проверьте статус:
        squeue
      Вы увидите свою задачу в очереди (статус PD - pending) или уже в работе (статус R - running).

   4. Посмотрите результат: После выполнения в директории появятся файлы test_conda_1gpu_*.out (стандартный вывод) и test_conda_1gpu_*.err (ошибки). Изучите .out файл — он должен показать, что скрипт обнаружил и использовал только один GPU.

  5. Дальнейшие шаги

   * Для своих задач создавайте копии run_test_conda_1gpu.sbatch и изменяйте их под свои нужды (путь к вашему скрипту, требуемые ресурсы).
   * Если вам нужны дополнительные пакеты, сначала активируйте свое окружение (conda activate ./my-torch-env), а затем устанавливайте их (conda install <пакет>).
