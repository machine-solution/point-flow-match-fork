# Память при обучении PointFlowMatch (open_fridge)

Краткий разбор, что держится в памяти и почему возможен CUDA OOM при воспроизведении настроек из конфига.

## Что в конфиге по умолчанию

- **batch_size:** 128  
- **n_obs_steps:** 2, **n_pred_steps:** 32  
- **n_points:** 4096 (точек на облако)  
- **use_ema:** True (второй набор весов для EMA)  
- **dataloader.num_workers:** в sbatch передаётся 8  

## Где что лежит

### Датасет — не целиком в памяти

Данные хранятся в zarr на диске (`demos/sim/open_fridge/{train,valid}`).  
`RobotDatasetPcd` открывает их через `RobotReplayBuffer.create_from_path(..., mode="r")`.  
В GPU/CPU одновременно попадают только те сэмплы, которые загружает DataLoader для текущего и следующих батчей (воркеры держат в очереди несколько батчей).  
То есть **весь датасет в оперативную память не загружается**.

### Один батч (размеры тензоров)

- **pcd:** `(B, n_obs_steps, n_points, 3)` = (128, 2, 4096, 3) float32 → **≈12.6 MB**  
- **robot_state_obs:** (128, 2, 10) → десятки KB  
- **robot_state_pred:** (128, 32, 10) → сотни KB  

Сами тензоры батча — порядка **~13 MB**. Основной потребитель VRAM — не они, а граф и активации в модели при forward/backward.

### Модель на GPU

- **Backbone (PointNet):** эмбеддинг 256, вход 2 шага × 4096 точек × 3 канала. Параметры — порядка нескольких миллионов.
- **ConditionalUnet1D:** `down_dims: [256, 512, 1024]`, вход по времени длины 32, `y_dim=10`, условие по наблюдениям. Тоже миллионы параметров.

Оценка суммарно: **десятки миллионов параметров** → модель в fp32 **сотни MB** (например 50M × 4 байта ≈ 200 MB).

### Оптимизатор и EMA

- **AdamW:** два состояния на параметр (momentum + variance) → ещё примерно **2× от размера модели** на GPU.  
- **use_ema: True** → копия весов модели для EMA → ещё **~1× размер модели**.  

Итого только веса + оптимизатор + EMA: порядка **~1 GB** и выше (зависит от точного числа параметров).

### Активации (главный потребитель VRAM)

При forward и backward PyTorch хранит промежуточные активации для градиентов.  
Для батча 128, двух облаков по 4096 точек и UNet с слоями 256→512→1024 объём активаций может составлять **несколько GB** (часто 5–15 GB и больше в зависимости от реализации).  
Именно это обычно и даёт OOM при batch_size=128 на GPU с 40 GB, если ещё есть общая память под другие процессы или фрагментация.

## Что пишут авторы про ресурсы

В README и конфигах репозитория **нет явных требований** к объёму VRAM или к batch size.  
Упоминается только запуск на Dexter (DGX A100). У A100 бывает 40 GB и 80 GB на устройство; при 40 GB и batch 128 с EMA и 8 воркерами запас по памяти небольшой.

## Что можно сделать без смены логики обучения

1. **Убедиться, что GPU не шарится:** в логе job’а посмотреть `CUDA_VISIBLE_DEVICES` и при необходимости уточнить у админов, не выделяется ли один и тот же GPU нескольким задачам.
2. **Запросить узел с A100 80GB**, если в кластере есть такая очередь/partition — тогда те же batch_size и настройки имеют больший запас.
3. **Чуть уменьшить потребление CPU/RAM от даталоадера:** в sbatch поставить `dataloader.num_workers=4` вместо 8 (на GPU память это не спасёт, но снизит пиковое использование RAM и число процессов).
4. **Если OOM остаётся** — единственный надёжный способ сохранить всё остальное (модель, EMA, логику) — уменьшить **batch_size** (например 64 или 32); в конфиге или через override в sbatch. Это ослабляет воспроизводимость только в смысле «строго тот же batch size», сама схема обучения и архитектура остаются теми же.

## Оценки в одну строку

| Что | Где | Порядок величины |
|-----|-----|------------------|
| Датасет | диск, подгружаются батчи | не весь в RAM |
| Один батч (тензоры) | GPU | ~13 MB |
| Модель + оптимизатор + EMA | GPU | ~1–2 GB |
| Активации (forward/backward) | GPU | несколько GB (основной потребитель) |

Итого при batch 128 и одной карте A100 40GB пиковое использование VRAM может приближаться к лимиту; на 80GB или при выделении целой карты без шаринга те же настройки обычно проходят.
